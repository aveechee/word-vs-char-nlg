{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "423aad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59106a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Avail:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\vrchee\\\\Documents\\\\MSDS\\\\NLP\\\\chee-v-hw-4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Num GPUs Avail: ',len(tf.config.list_physical_devices('GPU')))\n",
    "os.getcwd()\n",
    "#https://www.tensorflow.org/text/tutorials/text_generation\n",
    "#https://github.com/tensorflow/text/blob/master/docs/tutorials/text_generation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e04a7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_file='emma_book_only.txt'\n",
    "path_to_file='sanditon.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97687ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 129249 characters\n"
     ]
    }
   ],
   "source": [
    "text=open(path_to_file,'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "print(f'Length of text: {len(text)} characters') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9af0f62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1\r\n",
      "A gentleman and a lady travelling from Tunbridge towards that part of the Sussex coast which lies between Hastings and Eastbourne, being induced by business to quit the high road and attempt a very rough lane, were overturned in toiling up\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3fc99a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab=sorted(set(text))\n",
    "\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4c72fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(129249,), dtype=string, numpy=array([b'C', b'h', b'a', ..., b'a', b'm', b'.'], dtype=object)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Process Data'''\n",
    "\n",
    "chars=tf.strings.unicode_split(text, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdf7d4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars=preprocessing.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
    "\n",
    "ids=ids_from_chars(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcc90ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids=tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "chars=chars_from_ids(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bafad24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()\n",
    "\n",
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab2321a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n",
      "h\n",
      "a\n",
      "p\n",
      "t\n",
      "e\n",
      "r\n",
      " \n",
      "1\n",
      "\r\n",
      "tf.Tensor(\n",
      "[b'C' b'h' b'a' b'p' b't' b'e' b'r' b' ' b'1' b'\\r' b'\\n' b'A' b' ' b'g'\n",
      " b'e' b'n' b't' b'l' b'e' b'm' b'a' b'n' b' ' b'a' b'n' b'd' b' ' b'a'\n",
      " b' ' b'l' b'a' b'd' b'y' b' ' b't' b'r' b'a' b'v' b'e' b'l' b'l' b'i'\n",
      " b'n' b'g' b' ' b'f' b'r' b'o' b'm' b' ' b'T' b'u' b'n' b'b' b'r' b'i'\n",
      " b'd' b'g' b'e' b' ' b't' b'o' b'w' b'a' b'r' b'd' b's' b' ' b't' b'h'\n",
      " b'a' b't' b' ' b'p' b'a' b'r' b't' b' ' b'o' b'f' b' ' b't' b'h' b'e'\n",
      " b' ' b'S' b'u' b's' b's' b'e' b'x' b' ' b'c' b'o' b'a' b's' b't' b' '\n",
      " b'w' b'h' b'i'], shape=(101,), dtype=string)\n",
      "b'Chapter 1\\r\\nA gentleman and a lady travelling from Tunbridge towards that part of the Sussex coast whi'\n",
      "b'ch lies between Hastings and Eastbourne, being induced by business to quit the high road and attempt '\n",
      "b'a very rough lane, were overturned in toiling up its long ascent, half rock, half sand. The accident '\n",
      "b\"happened just beyond the only gentleman's house near the lane\\xe2\\x80\\x94a house which their driver, on being fi\"\n",
      "b'rst required to take that direction, had conceived to be necessarily their object and had with most u'\n",
      "Input : b'Chapter 1\\r\\nA gentleman and a lady travelling from Tunbridge towards that part of the Sussex coast wh'\n",
      "Target: b'hapter 1\\r\\nA gentleman and a lady travelling from Tunbridge towards that part of the Sussex coast whi'\n"
     ]
    }
   ],
   "source": [
    "'''Prediction Task'''\n",
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids\n",
    "\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "\n",
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n",
    "    \n",
    "\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_ids(seq))\n",
    "\n",
    "\n",
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())\n",
    "\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "\n",
    "split_input_target(list(\"Tensorflow\"))\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b74262dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Create Training batches'''\n",
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1258672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Build the Model'''\n",
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67363928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0825e233",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7eaa2fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 80) # (batch_size, sequence_length, vocab_size)\n",
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  20480     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    multiple                  3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  82000     \n",
      "=================================================================\n",
      "Total params: 4,040,784\n",
      "Trainable params: 4,040,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Input:\n",
      " b'is! It was impossible not to feel him hardly used: to be obliged to stand back in his own house and '\n",
      "\n",
      "Next Char Predictions:\n",
      " b'zu,jHPr22ymgpovHx\").\\xc3\\xaaPGTjEY!0n?-hvi- ih\\nkkUP?J4\\xc3\\xaa\\xc3\\xa0R0DH\\rjJy45\\'UcFtPh BoA\"R\\xc3\\xa8l;\\xc3\\xa0yN\\'GWk5DEEv:NqnxDq3yJbtn'\n"
     ]
    }
   ],
   "source": [
    "'''Try Model'''\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "    \n",
    "model.summary()\n",
    "\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices\n",
    "\n",
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ebea065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 80)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         4.382318\n"
     ]
    }
   ],
   "source": [
    "'''Train the Model'''\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "mean_loss = example_batch_loss.numpy().mean()\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54e537c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.exp(mean_loss).numpy()\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "600f315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc85b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e213f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "19/19 [==============================] - 2s 26ms/step - loss: 3.9563\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 2.9924: 0s - loss: 3.0\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 2.7088\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 2.4793\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 2.3582\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 2.2824\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 2.2253\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 2.1673\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 2.1107\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 2.0495\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - ETA: 0s - loss: 1.988 - 1s 26ms/step - loss: 1.9879\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 1s 25ms/step - loss: 1.9275\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 1.8706\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 1.8160: 0s - loss: 1.816\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 1.7599: 0s - loss: 1.76\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 1.7062\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 1.6533\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 1.6089: 0s - loss: 1.608\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 1.5592: 0s - los\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 1.5081\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 1.4662\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 1.4204: 0s - loss: 1. - ETA: 0s - loss: 1.420\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 1.3760\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 1.3338: 0s - loss: 1\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 1.2899\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 1.2488: 0s - loss: \n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 1.2079\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 1.1656\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 1.1281: 0s - loss: 1.1 - ETA: 0s - loss: 1.12\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 1.0821\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 1.0395: 0s - loss: 1\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 0.9940: 0s - loss: 0.9\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 0.9490\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 0.9052: 0s - los\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 0.8550\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 0.8026\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 0.7522: 0s - loss: 0. - ETA: 0s - loss: 0.752\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 0.6991: 0s - loss: \n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 0.6472\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 0.5980: 0s - loss: 0.\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 0.5480: 0s - loss: 0\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 0.4913\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 0.4397\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 0.3921: 0s - loss: 0.3\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 0.3486: 0s - loss: 0\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 0.3075: 0s - loss: 0.3\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 0.2695\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 0.2365\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 0.2058: 0s - loss: 0.2\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 1s 27ms/step - loss: 0.1801: 0s - loss: 0 - ETA: 0s - loss: 0.17 - ETA: 0s - loss: 0.18\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1d23e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GENERATE TEXT'''\n",
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put a -inf at each bad index.\n",
    "            values=[-float('inf')]*len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "    \n",
    "        # Run the model.\n",
    "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "        predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                              return_state=True)\n",
    "        # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits/self.temperature\n",
    "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "    \n",
    "        # Sample the output logits to generate token IDs.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "    \n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "    \n",
    "        # Return the characters and model state.\n",
    "        return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67a456cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9d9f393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was impossible not to feel him hardly used: to be obliged to stand back in his own house and see the best place by the fire constantly occupied by Sir Harry Denham.\"\r\n",
      "\r\n",
      "\"And his house wese to make as tall a place in tawing-to her; and my dear Tom, upon I have no fancy for herself. Not as well as I have. And I do believe those are best on fide with her relation for tuins, the poor to fancy themselves or spirits to speak in a common. I should like to have you two,\r\n",
      "\r\n",
      "Mrs. Parker, who, in her first really volusemes of probably steps at Sanditon, it would be better if they would leave themselves undeasonably indisposed to Vielly ojje's account. have such fortitude!\" and Charlotte asseding anything more than anything. I have viewede and just as well be done.\"\r\n",
      "\r\n",
      "\"Oh, me a was evidently a sentance like Sanditon. The sea air would probubly be the death of me. And neither do every portrywloms! Hervey were extreme young man, however, and that but not to add the empassood of Sid Edward's right to invite one of the parishâ€”end; and though within the house.) \"I began to wined and good nourishmesting into her coampsticonisifing and consulting by its size from  \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.9333224296569824\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['It was impossible not to feel him hardly used: to be obliged to stand back in his own house and see the best place by the fire constantly occupied by Sir Harry Denham.'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87427141",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Export the Generator'''\n",
    "#tf.saved_model.save(one_step_model, 'one_step')\n",
    "#one_step_reloaded = tf.saved_model.load('one_step')\n",
    "#\n",
    "#states = None\n",
    "#next_char = tf.constant(['ROMEO:'])\n",
    "#result = [next_char]\n",
    "#\n",
    "#for n in range(100):\n",
    "#    next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "#    result.append(next_char)\n",
    "#\n",
    "#print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eafd1a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4228ad0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
