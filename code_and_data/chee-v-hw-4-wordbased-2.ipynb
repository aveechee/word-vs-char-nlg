{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5299ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''FUNCTIONS'''\n",
    "import pickle\n",
    "from string import ascii_lowercase, digits\n",
    "#from bs4 import BeautifulSoup, NavigableString\n",
    "from collections import Counter\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "# Loading and saving files\n",
    "\n",
    "def read_txt(path):\n",
    "    return open(path, 'r', encoding='utf-8').read()\n",
    "\n",
    "def save_txt(text, path):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as handle:\n",
    "        return pickle.load(handle)\n",
    "\n",
    "def save_pickle(variable, path):\n",
    "    with open(path, 'wb') as handle:\n",
    "        pickle.dump(variable, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11967813",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TEXT FUNCTIONS'''\n",
    "\n",
    "import re\n",
    "\n",
    "class Text:\n",
    "    def __init__(self, input_text, token2ind=None, ind2token=None):\n",
    "        self.content = input_text\n",
    "        self.tokens, self.tokens_distinct = self.tokenize()\n",
    "\n",
    "        if token2ind != None and ind2token != None:\n",
    "            self.token2ind, self.ind2token = token2ind, ind2token\n",
    "        else:\n",
    "            self.token2ind, self.ind2token = self.create_word_mapping(self.tokens_distinct)\n",
    "\n",
    "        self.tokens_ind = [self.token2ind[token] if token in self.token2ind.keys() else self.token2ind['<| unknown |>']\n",
    "                           for token in self.tokens]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.content\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens_distinct)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_word_mapping(values_list):\n",
    "        values_list.append('<| unknown |>')\n",
    "        value2ind = {value: ind for ind, value in enumerate(values_list)}\n",
    "        ind2value = dict(enumerate(values_list))\n",
    "        return value2ind, ind2value\n",
    "\n",
    "    def preprocess(self):\n",
    "        punctuation_pad = '!?.,:-;'\n",
    "        punctuation_remove = '\"()_\\n'\n",
    "\n",
    "        self.content_preprocess = re.sub(r'(\\S)(\\n)(\\S)', r'\\1 \\2 \\3', self.content)\n",
    "        self.content_preprocess = self.content_preprocess.translate(str.maketrans('', '', punctuation_remove))\n",
    "        self.content_preprocess = self.content_preprocess.translate(\n",
    "            str.maketrans({key: ' {0} '.format(key) for key in punctuation_pad}))\n",
    "        self.content_preprocess = re.sub(' +', ' ', self.content_preprocess)\n",
    "        self.content = self.content_preprocess.strip()\n",
    "\n",
    "    def tokenize(self):\n",
    "        self.preprocess()\n",
    "        tokens = self.content.split(' ')\n",
    "        return tokens, list(set(tokens))\n",
    "\n",
    "    def tokens_info(self):\n",
    "        print('total tokens: %d, distinct tokens: %d' % (len(self.tokens), len(self.tokens_distinct)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7afe3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''LSTM FUNCTIONS'''\n",
    "#import re\n",
    "import numpy as np\n",
    "import tensorflow.keras\n",
    "\n",
    "class Sequences():\n",
    "    def __init__(self, text_object, max_len, step):\n",
    "        self.tokens_ind = text_object.tokens_ind\n",
    "        self.max_len = max_len\n",
    "        self.step = step\n",
    "        self.sequences, self.next_words = self.create_sequences()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Sequence object of max_len: %d and step: %d' % (self.max_len, self.step)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def create_sequences(self):\n",
    "        sequences = []\n",
    "        next_words = []\n",
    "\n",
    "        for i in range(0, len(self.tokens_ind) - self.max_len, self.step):\n",
    "            sequences.append(self.tokens_ind[i: i +self.max_len])\n",
    "            next_words.append(self.tokens_ind[ i +self.max_len])\n",
    "        return sequences, next_words\n",
    "\n",
    "    def sequences_info(self):\n",
    "        print('number of sequences of length %d: %d' % (self.max_len, len(self.sequences)))\n",
    "\n",
    "\n",
    "class ModelPredict():\n",
    "    def __init__(self, model, prefix, token2ind, ind2token, max_len, embedding=False):\n",
    "        self.model = model\n",
    "        self.token2ind, self.ind2token = token2ind, ind2token\n",
    "        self.max_len = max_len\n",
    "        self.prefix = prefix\n",
    "        self.tokens_ind = prefix.tokens_ind.copy()\n",
    "        self.embedding = embedding\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.prefix.content\n",
    "\n",
    "    def single_data_generation(self):\n",
    "        single_sequence = np.zeros((1, self.max_len, len(self.token2ind)), dtype=np.bool)\n",
    "        prefix = self.tokens_ind[-self.max_len:]\n",
    "\n",
    "        for i, s in enumerate(prefix):\n",
    "            single_sequence[0, i, s] = 1\n",
    "        return single_sequence\n",
    "\n",
    "    def model_predict(self):\n",
    "        if self.embedding:\n",
    "            model_input = np.array(self.tokens_ind).reshape(1,-1)\n",
    "        else:\n",
    "            model_input = self.single_data_generation()\n",
    "        return self.model.predict(model_input)[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def add_prob_temperature(prob, temperature=1):\n",
    "        prob = prob.astype(float)\n",
    "        prob_with_temperature = np.exp(np.where(prob == 0, 0, np.log(prob + 1e-10)) / temperature)\n",
    "        prob_with_temperature /= np.sum(prob_with_temperature)\n",
    "        return prob_with_temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def reverse_preprocess(text):\n",
    "        text_reverse = re.sub(r'\\s+([!?\"\\'().,;-])', r'\\1', text)\n",
    "        text_reverse = re.sub(' +', ' ', text_reverse)\n",
    "        return text_reverse\n",
    "\n",
    "    def return_next_word(self, temperature=1, as_word=False):\n",
    "        prob = self.model_predict()\n",
    "\n",
    "        prob_with_temperature = self.add_prob_temperature(prob, temperature)\n",
    "        next_word = np.random.choice(len(prob_with_temperature), p=prob_with_temperature)\n",
    "\n",
    "        if as_word:\n",
    "            return self.ind2token[next_word]\n",
    "        else:\n",
    "            return next_word\n",
    "\n",
    "    def generate_sequence(self, k, append=False, temperature=1):\n",
    "        for i in range(k):\n",
    "            next_word = self.return_next_word(temperature=temperature)\n",
    "            self.tokens_ind.append(next_word)\n",
    "        return_tokens_ind = self.tokens_ind\n",
    "        return_tokens_ind = ' '.join([self.ind2token[ind] for ind in return_tokens_ind])\n",
    "\n",
    "        if not append:\n",
    "            self.tokens_ind = self.prefix.tokens_ind.copy()\n",
    "\n",
    "        return self.reverse_preprocess(return_tokens_ind)\n",
    "\n",
    "    def bulk_generate_sequence(self, k, n, temperature=1):\n",
    "        for i in range(n):\n",
    "            print(self.generate_sequence(k, temperature=temperature))\n",
    "            print('\\n')\n",
    "\n",
    "\n",
    "class TextDataGenerator(Sequence):\n",
    "    def __init__(self, sequences, next_words, sequence_length, vocab_size, batch_size=32, shuffle=True, embedding=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.sequences = sequences\n",
    "        self.next_words = next_words\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.shuffle = shuffle\n",
    "        self.embedding = embedding\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.sequences) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "        sequences_batch = [self.sequences[k] for k in indexes]\n",
    "        next_words_batch = [self.next_words[k] for k in indexes]\n",
    "\n",
    "        if self.embedding:\n",
    "            X = np.array(sequences_batch)\n",
    "            y = keras.utils.to_categorical(next_words_batch, num_classes=self.vocab_size)\n",
    "        else:\n",
    "            X, y = self.__data_generation(sequences_batch, next_words_batch)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.sequences))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, sequences_batch, next_words_batch):\n",
    "        X = np.zeros((self.batch_size, self.sequence_length, self.vocab_size), dtype=np.bool)\n",
    "        y = np.zeros((self.batch_size, self.vocab_size), dtype=np.bool)\n",
    "\n",
    "        for i, seq in enumerate(sequences_batch):\n",
    "            for j, word in enumerate(seq):\n",
    "                X[i, j, word] = 1\n",
    "                y[i, next_words_batch[i]] = 1\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79bc2ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens: 185790, distinct tokens: 9861\n",
      "number of sequences of length 4: 61929\n",
      "['\\ufeffCHAPTER', 'IEmma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich']\n",
      "[6211, 112, 4876, 9442, 6379, 9442, 1997, 9442, 9635, 2602] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''START HERE'''\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "'''PREPROCESSING'''\n",
    "path_train = 'emma_book_only.txt'\n",
    "\n",
    "input_train = read_txt(path_train)\n",
    "\n",
    "\n",
    "max_len = 4\n",
    "step = 3\n",
    "\n",
    "text_train = Text(input_train)\n",
    "text_train.tokens_info()\n",
    "\n",
    "seq_train = Sequences(text_train, max_len, step)\n",
    "seq_train.sequences_info()\n",
    "\n",
    "\n",
    "print(text_train.tokens[:10])\n",
    "print(text_train.tokens_ind[:10], '\\n')\n",
    "\n",
    "np.array(seq_train.sequences[:2])\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "params = {\n",
    "  'sequence_length': max_len,\n",
    "  'vocab_size': len(text_train),\n",
    "  'batch_size': batch_size,\n",
    "  'shuffle': True\n",
    "}\n",
    "\n",
    "train_generator = TextDataGenerator(seq_train.sequences, seq_train.next_words, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84b88d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TRAIN LSTM MODEL'''\n",
    "def lstm_model(sequence_length, vocab_size, layer_size, embedding=False):\n",
    "    model = models.Sequential()\n",
    "    if embedding:\n",
    "        model.add(layers.Embedding(vocab_size, layer_size))\n",
    "        model.add(layers.LSTM(layer_size))    \n",
    "    else:\n",
    "        model.add(layers.LSTM(layer_size, input_shape=(sequence_length, vocab_size)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(vocab_size, activation='softmax'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "937ee818",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_model(max_len, len(text_train), 32)\n",
    "\n",
    "optimizer = optimizers.RMSprop(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cbb6873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrchee\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:134: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "C:\\Users\\vrchee\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:135: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1935/1935 [==============================] - 11s 4ms/step - loss: 6.3241\n",
      "Epoch 2/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7439\n",
      "Epoch 3/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7716\n",
      "Epoch 4/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7734\n",
      "Epoch 5/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7829\n",
      "Epoch 6/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7635\n",
      "Epoch 7/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7612\n",
      "Epoch 8/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7515\n",
      "Epoch 9/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7406\n",
      "Epoch 10/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7194\n",
      "Epoch 11/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7158\n",
      "Epoch 12/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7116\n",
      "Epoch 13/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7150\n",
      "Epoch 14/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7215\n",
      "Epoch 15/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7672\n",
      "Epoch 16/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7673\n",
      "Epoch 17/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7559\n",
      "Epoch 18/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7788\n",
      "Epoch 19/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7895\n",
      "Epoch 20/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7592\n",
      "Epoch 21/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7538\n",
      "Epoch 22/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7401\n",
      "Epoch 23/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7378\n",
      "Epoch 24/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7329\n",
      "Epoch 25/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7454\n",
      "Epoch 26/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7615\n",
      "Epoch 27/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7303\n",
      "Epoch 28/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7151\n",
      "Epoch 29/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7406\n",
      "Epoch 30/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7335\n",
      "Epoch 31/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7064\n",
      "Epoch 32/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7409\n",
      "Epoch 33/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7510\n",
      "Epoch 34/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7205\n",
      "Epoch 35/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7064\n",
      "Epoch 36/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.6782\n",
      "Epoch 37/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.6877\n",
      "Epoch 38/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7155\n",
      "Epoch 39/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7143\n",
      "Epoch 40/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7042\n",
      "Epoch 41/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.6806\n",
      "Epoch 42/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.6629\n",
      "Epoch 43/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.6638\n",
      "Epoch 44/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.6525\n",
      "Epoch 45/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.6658\n",
      "Epoch 46/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7080\n",
      "Epoch 47/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.7063\n",
      "Epoch 48/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.6689\n",
      "Epoch 49/50\n",
      "1935/1935 [==============================] - 9s 4ms/step - loss: 6.6519\n",
      "Epoch 50/50\n",
      "1935/1935 [==============================] - 8s 4ms/step - loss: 6.6231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1942278e308>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator,\n",
    "          steps_per_epoch=len(train_generator),\n",
    "          epochs=50,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0e42433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('data/out/lstm_model')\n",
    "#model = models.load_model('data/out/lstm_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a79b150c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrchee\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elizabeth heard; : well the. to be to be very much as to It was a the so to Emma! and had a of a very good and about the the been, and the as a of his very; and that he could not have been a in she. The to be, in him to be, but you must have a very angry as for a than I seemed as I am sure that friend for the. and. Mr. Elton, as Mr. she might be at that a more to be very much a much to be very much in a very well at much of the. It was so in Emma. The it would have been in, he did have to of me. ” said he, the is to have not one of the Mr. Knightley, it would have been a; and she had been to be as Mr. Mr. ” said him; that she had been in all the her. She she could not would have here the very good the very. Mr\n",
      "\n",
      "\n",
      "temperature: 0.7\n",
      "Elizabeth if quite every to and had own with the a very a very; and in Mr. she might be very much. Mr. Knightley. He was very very much in a to his own. It was so to be the very to to a be at all or thing, as I am sure you were so good. He do her to am to send, there Emma never would The had Miss Woodhouse, Miss Woodhouse. He could not be to be in the very to the very to the to. She had been very Mrs. Elton, that she had heard to be to the much at all the a a little and be to be much of a much. She had in a very great deal of his a very of a very little of; and so, she is so, and you, that it is so much to a, that a great a most Mr. Elton was, that Miss Fairfax was it is. She was very much that he would have been with her; and he could not\n",
      "\n",
      "\n",
      "temperature: 0.4\n",
      "Elizabeth there we you will be by my Mr. Woodhouse, very much as. he had had be sure, and Mr. Elton was to be. ”“I am sure I will be so good very the few aware of and he had have been to be with it is Mr. Elton not be so much; and she had not to to you. The most, that nothing, it was much so much; and she was so to have a little. She was to not know his Miss Woodhouse, indeed much of my as Emma of the own little had not been very being in his; but in of the. She was at all the a very very his in a very little as in the very little, and in the as Mr. ”Emma could be to be as so much at any thing, she would be a little, and very, and you have been, and my the very; and being. The a little, for you, even this, at in her. She was a very much of\n",
      "\n",
      "\n",
      "temperature: 0.1\n",
      "Elizabeth cannot their you should be, on my very a, ” said he, Mr. Woodhouse, with Mr. Elton in his much that he is to be the to to her to being. It was not she had been to be of a very to be of her own of being a very a very to be. Mr. Elton, but at all at. She is a very much one of the most as she had been, Mr. Elton, as him. The Mr. Weston, which I would have have been a of a and I am sure you are so to be. to be of it to see Mr. He might to Mr. good as he had to, Mr. Knightley, Mrs. Weston, as it would be a much as Mr. Knightley he will not be to have you must. He were very a to a good. He must be very good. The very of his very much to the Mr. Elton. The very her the very of the all. She was\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''TEXT GENERATION'''\n",
    "token2ind, ind2token = text_train.token2ind, text_train.ind2token\n",
    "\n",
    "input_prefix = 'Elizabeth'\n",
    "text_prefix = Text(input_prefix, token2ind, ind2token)\n",
    "\n",
    "\n",
    "pred = ModelPredict(model, text_prefix, token2ind, ind2token, max_len)\n",
    "\n",
    "\n",
    "temperatures = [1, 0.7, 0.4, 0.1]\n",
    "\n",
    "for temperature in temperatures:\n",
    "    print('temperature:', temperature)\n",
    "    print(pred.generate_sequence(200, temperature=0.7))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff73127",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
